{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c5b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda\n",
      "총 VRAM: 23.57 GB. ViT-7B 로드를 위해 FP16/BF16이 필수입니다.\n",
      "\n",
      "--- DINOv3 AutoModel 성능 및 속도 비교 시작 ---\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vit7b16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fcc2d4b9bc48f49fc4da7e4a28d63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d0cc04bc454425bf7f1f51bd6c7889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 66.719 ms\n",
      "  추출 특징 형태: (1, 201, 4096)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vith16plus-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de1dac13e3a4f8fa9277052b8513455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 21.643 ms\n",
      "  추출 특징 형태: (1, 201, 1280)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vitl16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0105e703822a4522ba33004e7f68bbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 19.433 ms\n",
      "  추출 특징 형태: (1, 201, 1024)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vitb16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dd3c7fe8bb4d13a9d9cd3669e35e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 9.939 ms\n",
      "  추출 특징 형태: (1, 201, 768)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vits16plus-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa17272361e49f1a0bbd1e2accbdf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 8.270 ms\n",
      "  추출 특징 형태: (1, 201, 384)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-vits16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf84b17c2b045aa9fd37294c3e9ab9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 8.102 ms\n",
      "  추출 특징 형태: (1, 201, 384)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-convnext-large-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836f950fd3374aa198d8fd6c707b8f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 7.411 ms\n",
      "  추출 특징 형태: (1, 50, 1536)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-convnext-base-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c919f8bc80438b87d18033759c8cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 7.463 ms\n",
      "  추출 특징 형태: (1, 50, 1024)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-convnext-small-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2212b6176a844b0b629d2fe252685ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 7.484 ms\n",
      "  추출 특징 형태: (1, 50, 768)\n",
      "\n",
      "[모델 로딩]: facebook/dinov3-convnext-tiny-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0410466190435f8c167c690a1bc8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 로딩 성공!\n",
      "  평균 지연 시간: 4.092 ms\n",
      "  추출 특징 형태: (1, 50, 768)\n",
      "\n",
      "--- 최종 비교 결과 ---\n",
      "Model                                    |     Avg Latency (ms) |        Feature Shape\n",
      "-------------------------------------------------------------------------------------\n",
      "dinov3-vit7b16-pretrain-lvd1689m         |               66.719 |       (1, 201, 4096)\n",
      "dinov3-vith16plus-pretrain-lvd1689m      |               21.643 |       (1, 201, 1280)\n",
      "dinov3-vitl16-pretrain-lvd1689m          |               19.433 |       (1, 201, 1024)\n",
      "dinov3-vitb16-pretrain-lvd1689m          |                9.939 |        (1, 201, 768)\n",
      "dinov3-vits16plus-pretrain-lvd1689m      |                8.270 |        (1, 201, 384)\n",
      "dinov3-vits16-pretrain-lvd1689m          |                8.102 |        (1, 201, 384)\n",
      "dinov3-convnext-large-pretrain-lvd1689m  |                7.411 |        (1, 50, 1536)\n",
      "dinov3-convnext-base-pretrain-lvd1689m   |                7.463 |        (1, 50, 1024)\n",
      "dinov3-convnext-small-pretrain-lvd1689m  |                7.484 |         (1, 50, 768)\n",
      "dinov3-convnext-tiny-pretrain-lvd1689m   |                4.092 |         (1, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "# 1. 비교 대상 모델 리스트 (7B 포함)\n",
    "MODELS_TO_COMPARE = [\n",
    "    \"facebook/dinov3-vit7b16-pretrain-lvd1689m\", # 7B 모델 (FP16/BF16 필수)\n",
    "    \"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-vitl16-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-vits16plus-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-convnext-large-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-convnext-base-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-convnext-small-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n",
    "]\n",
    "\n",
    "# 2. 이미지 로드 및 장치 설정 (이전과 동일)\n",
    "IMAGE_PATH = \"/home/najo/NAS/DIP/2025_ICRA_Multi_View_Robot_Pose_Estimation/dataset/franka_research3/franka_research3_pose1/Panda_dataset_1th/view1/zed_41182735_left_1756275914.742.jpg\"\n",
    "try:\n",
    "    image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "except FileNotFoundError:\n",
    "    # 파일이 없는 경우, 가상의 이미지로 대체하여 코드 실행이 가능하도록 함\n",
    "    print(f\"오류: 이미지를 찾을 수 없습니다. 경로를 확인하세요: {IMAGE_PATH}\")\n",
    "    image = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8)).convert(\"RGB\")\n",
    "    print(\"가상의 224x224 이미지로 테스트를 진행합니다.\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 장치: {DEVICE}\")\n",
    "\n",
    "# 3. 모델별 성능 비교 루프\n",
    "results = []\n",
    "NUM_WARMUP = 5\n",
    "NUM_RUNS = 20\n",
    "\n",
    "# VRAM 정보를 확인하여 7B 모델 로딩 가능성 경고\n",
    "if DEVICE == \"cuda\":\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"총 VRAM: {total_vram:.2f} GB. ViT-7B 로드를 위해 FP16/BF16이 필수입니다.\")\n",
    "\n",
    "print(\"\\n--- DINOv3 AutoModel 성능 및 속도 비교 시작 ---\")\n",
    "\n",
    "for model_name in MODELS_TO_COMPARE:\n",
    "    print(f\"\\n[모델 로딩]: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 3-1. 모델과 전처리기(Image Processor) 로드\n",
    "        processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        # BF16으로 모델 로드 (7B 포함 모든 모델에 대해 최적의 추론 속도 및 메모리 사용을 위해)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.bfloat16,  # BF16 사용\n",
    "            low_cpu_mem_usage=True      \n",
    "        ).to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e) or \"Input type (float) and bias type\" in str(e):\n",
    "             # 메모리 부족 또는 이전 FP32 입력 오류 처리 (BF16으로 해결되었을 가능성이 높음)\n",
    "            print(\"❌ 로드 실패: VRAM 부족 (7B 모델일 가능성 높음) 또는 타입 불일치. 다음 모델로 넘어갑니다.\")\n",
    "            continue\n",
    "        else:\n",
    "             print(f\"❌ 로드 실패 (RuntimeError): {e}\")\n",
    "             continue\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 로드 실패 (AutoModel 문제): {e}\")\n",
    "        continue\n",
    "    \n",
    "    # 3-2. 이미지 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # [수정된 핵심 로직]: 입력 텐서의 타입을 모델의 타입(BF16)과 일치시키고 GPU로 전송\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(torch.bfloat16).to(DEVICE) \n",
    "        \n",
    "    # 3-3. 워밍업 (GPU/CPU 캐시 최적화)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_WARMUP):\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # 3-4. 실제 추론 시간 측정 (이전과 동일)\n",
    "    latencies = []\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_RUNS):\n",
    "            start_run = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            end_run = time.time()\n",
    "            latencies.append(end_run - start_run)\n",
    "\n",
    "    # 3-5. 결과 정리\n",
    "    avg_latency_ms = (sum(latencies) / NUM_RUNS) * 1000\n",
    "    \n",
    "    feature_tensor = outputs.last_hidden_state if 'last_hidden_state' in outputs else outputs[0]\n",
    "    feature_shape = tuple(feature_tensor.shape)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name.split('/')[-1],\n",
    "        \"Avg Latency (ms)\": f\"{avg_latency_ms:.3f}\",\n",
    "        \"Feature Shape\": feature_shape\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✅ 로딩 성공!\")\n",
    "    print(f\"  평균 지연 시간: {avg_latency_ms:.3f} ms\")\n",
    "    print(f\"  추출 특징 형태: {feature_shape}\")\n",
    "    \n",
    "    # 메모리 해제 (7B 모델 로드 후 메모리 부족 방지)\n",
    "    del model\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# 4. 최종 결과 출력 (이전과 동일)\n",
    "print(\"\\n--- 최종 비교 결과 ---\")\n",
    "if results:\n",
    "    # 모델 크기별 순서 (7B 추가)\n",
    "    order = [\n",
    "        \"vit7b16\", \"vith16plus\", \"vitl16\", \"vitb16\", \"vits16plus\", \"vits16\", \n",
    "        \"convnext-large\", \"convnext-base\", \"convnext-small\", \"convnext-tiny\"\n",
    "    ]\n",
    "    \n",
    "    def get_order_key(model_id):\n",
    "        full_name = model_id['Model']\n",
    "        if 'vit' in full_name:\n",
    "            name_part = full_name.split('-')[1]\n",
    "        elif 'convnext' in full_name:\n",
    "            name_parts = full_name.split('-')\n",
    "            name_part = f\"{name_parts[1]}-{name_parts[2]}\" \n",
    "        else:\n",
    "            return len(order) \n",
    "\n",
    "        try:\n",
    "            return order.index(name_part)\n",
    "        except ValueError:\n",
    "            return len(order) \n",
    "            \n",
    "    sorted_results = sorted(results, key=get_order_key)\n",
    "\n",
    "    print(f\"{'Model':<40} | {'Avg Latency (ms)':>20} | {'Feature Shape':>20}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for res in sorted_results:\n",
    "        shape_str = str(res['Feature Shape'])\n",
    "        print(f\"{res['Model']:<40} | {res['Avg Latency (ms)']:>20} | {shape_str:>20}\")\n",
    "else:\n",
    "    print(\"비교할 결과를 얻지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a957ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56a3a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[어텐션 맵 추출]: facebook/dinov3-vit7b16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3872a3eb9834eae8774761e56ee0f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee46a607c4478cb2a0c63c7025af3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로딩 실패: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 30.44 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 23.27 GiB is allocated by PyTorch, and 943.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "[어텐션 맵 추출]: facebook/dinov3-vitb16-pretrain-lvd1689m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222bd36cc6d47dd937bbe88f3ef1583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m     visualize_attention(att_map_L, image, name_L)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# 2. ViT-B16 분석\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m att_map_B, name_B = \u001b[43mextract_attention_map\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/dinov3-vitb16-pretrain-lvd1689m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m att_map_B \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m     visualize_attention(att_map_B, image, name_B)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mextract_attention_map\u001b[39m\u001b[34m(model_id, image, device)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 3. 추론 및 어텐션 가중치 추출\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# DINOv3 ViT 모델은 'attentions' 키를 통해 가중치 튜플을 반환합니다.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 각 요소는 (Batch, Head, Seq_Len, Seq_Len) 형태입니다.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 일반적으로 마지막 레이어의 어텐션 가중치를 사용합니다.\u001b[39;00m\n\u001b[32m     35\u001b[39m attention_weights_list = outputs.attentions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/transformers/models/dinov3_vit/modeling_dinov3_vit.py:518\u001b[39m, in \u001b[36mDINOv3ViTModel.forward\u001b[39m\u001b[34m(self, pixel_values, bool_masked_pos, head_mask, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    512\u001b[39m \u001b[33;03mbool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Only relevant for\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    pre-training.\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    517\u001b[39m pixel_values = pixel_values.to(\u001b[38;5;28mself\u001b[39m.embeddings.patch_embeddings.weight.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    519\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rope_embeddings(pixel_values)\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/transformers/models/dinov3_vit/modeling_dinov3_vit.py:60\u001b[39m, in \u001b[36mDINOv3ViTEmbeddings.forward\u001b[39m\u001b[34m(self, pixel_values, bool_masked_pos)\u001b[39m\n\u001b[32m     57\u001b[39m target_dtype = \u001b[38;5;28mself\u001b[39m.patch_embeddings.weight.dtype\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# (batch_size, num_channels, height, width) -> (batch_size, num_patches, hidden_size)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m patch_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m patch_embeddings = patch_embeddings.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dinov3/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_attention_map(model_id, image, device):\n",
    "    print(f\"\\n[어텐션 맵 추출]: {model_id}\")\n",
    "    \n",
    "    # 1. 모델과 프로세서 로드 (output_attentions=True 설정)\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "        # AutoModel 대신 특정 ViT 클래스를 사용하거나, AutoModel의 설정(config)를 수정해야 합니다.\n",
    "        # DINOv3는 AutoModel로 로드 가능하지만, Hugging Face 모델이 output_attentions=True를 기본적으로 제공하지 않을 수 있습니다.\n",
    "        \n",
    "        # 안전을 위해 AutoModel.from_pretrained에서 직접 설정 전달\n",
    "        model = AutoModel.from_pretrained(model_id, output_attentions=True)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"로딩 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. 이미지 전처리\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 3. 추론 및 어텐션 가중치 추출\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # DINOv3 ViT 모델은 'attentions' 키를 통해 가중치 튜플을 반환합니다.\n",
    "    # 각 요소는 (Batch, Head, Seq_Len, Seq_Len) 형태입니다.\n",
    "    # 일반적으로 마지막 레이어의 어텐션 가중치를 사용합니다.\n",
    "    attention_weights_list = outputs.attentions\n",
    "    \n",
    "    if not attention_weights_list:\n",
    "        print(\"경고: output_attentions=True 옵션이 적용되지 않았거나 지원되지 않습니다.\")\n",
    "        return None, None\n",
    "\n",
    "    # 마지막 레이어의 어텐션 가중치 (Attention Weights from the last layer)\n",
    "    # shape: [Batch=1, Num_Heads, Seq_Len, Seq_Len]\n",
    "    last_layer_attention = attention_weights_list[-1][0].cpu().numpy()\n",
    "    \n",
    "    return last_layer_attention, model_id.split('/')[-1]\n",
    "\n",
    "def visualize_attention(attention_map, image, model_name):\n",
    "    # CLS 토큰 (첫 번째 인덱스)에서 다른 모든 토큰으로의 어텐션 가중치를 사용\n",
    "    # attention_map shape: [Num_Heads, Seq_Len, Seq_Len]\n",
    "    # CLS 토큰에서 공간 토큰으로의 어텐션 가중치 (Seq_Len - 1)\n",
    "    cls_attention = attention_map[:, 0, 1:].mean(axis=0) # [Seq_Len - 1] (현재 200)\n",
    "    \n",
    "    seq_len_spatial = cls_attention.shape[0] # 200\n",
    "\n",
    "    # 1. 그리드 사이즈 추정: 200에 가장 가까운 정수 제곱수 (14*14=196, 15*15=225)를 사용하여 근사\n",
    "    # DINOv3 ViT-B16의 표준 해상도는 14x14=196 이므로, 200개 중 196개만 사용하도록 강제함\n",
    "    grid_size = 14 # 표준 ViT-B16 패치 수의 제곱근\n",
    "    target_seq_len = grid_size * grid_size # 196\n",
    "    \n",
    "    if seq_len_spatial > target_seq_len:\n",
    "        # 시퀀스 길이가 196보다 크면 (현재 200), 초과된 토큰을 버립니다.\n",
    "        att_map_1d = cls_attention[:target_seq_len]\n",
    "        print(f\"  주의: 시퀀스 길이 {seq_len_spatial} 중 {target_seq_len}개 토큰만 사용.\")\n",
    "    elif seq_len_spatial < target_seq_len:\n",
    "        print(f\"시각화 오류: 시퀀스 길이가 196 미만입니다.\")\n",
    "        return\n",
    "    else:\n",
    "        att_map_1d = cls_attention\n",
    "        \n",
    "    # 2. 2D 맵으로 재구성 및 리사이징\n",
    "    att_map_2d = att_map_1d.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # 3. 시각화\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # 히트맵을 원본 이미지 크기에 맞춰 부드럽게 오버레이\n",
    "    plt.imshow(att_map_2d, cmap='viridis', alpha=0.6, \n",
    "               extent=[0, image.width, image.height, 0], interpolation='bilinear')\n",
    "    \n",
    "    plt.title(f'Attention Map (CLS Token): {model_name}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# --- 실행 예시 ---\n",
    "# (이전에 로드한 image와 DEVICE 변수가 정의되어 있다고 가정)\n",
    "IMAGE_PATH = \"/home/najo/NAS/DIP/2025_ICRA_Multi_View_Robot_Pose_Estimation/dataset/franka_research3/franka_research3_pose1/Panda_dataset_1th/view1/zed_41182735_left_1756275914.742.jpg\"\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. ViT-L16 분석\n",
    "att_map_L, name_L = extract_attention_map(\"facebook/dinov3-vit7b16-pretrain-lvd1689m\", image, DEVICE)\n",
    "if att_map_L is not None:\n",
    "    visualize_attention(att_map_L, image, name_L)\n",
    "\n",
    "# 2. ViT-B16 분석\n",
    "att_map_B, name_B = extract_attention_map(\"facebook/dinov3-vitb16-pretrain-lvd1689m\", image, DEVICE)\n",
    "if att_map_B is not None:\n",
    "    visualize_attention(att_map_B, image, name_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2431c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
